# Introduction

This document looks at five different way to provision cloud infrastructure. The goal is to get a better understand of the different methods and decide on the most suitable way to do so. Each method builds the same simple two-tier infrastructure on AWS.

This is not an introduction to AWS or a detailed introduction on how each of the discussed method works. The reader is expect to have a working understanding of AWS and Docker to follow the document. 

Each method builds the same infrastructure. You should destroy the infrastructure before moving on to the next method. You may occur cost if you keep the infrastructure running. 
The Docker files use the Ubuntu package mirror in New Zealand. The scripts use the AWS Sydney region (ap-southeast) and the user "lab01". You will need to adjust to suit your setup. If you adjust the region, you may need to adjust the AMI images too.

# Infrastructure

The following diagram shows the infrastructure. The infrastructure design can be improved and hardened but it suffices for this document.

![JWT flow](https://github.com/m3ccanico/blog/blob/master/003/diagram.svg)

It consists of
* 1 VPC
* 1 internet gateway
* 4 subnets
* 4 EC2 instances
* 1 routing table
* 3 security groups
* 2 availability zones

Each lab uses a different way to build the same infrastructure. This helps getting an initial understanding of each approach. Further, it allows comparing them against each other and helps identifying suitable use cases.

# Bash

## Execution

Create the Docker image:

```bash
docker build -f awscli.docker -t myawscli .
```

Run the container:

```bash
docker container run -it --name awscli --rm \
  --mount src=/<path-to-blog>/blog,target=/root/blog,type=bind \
  --mount src=/<path-to-aws-settings/.aws,target=/root/.aws,type=bind \
  myawscli /bin/bash
```

Set the correct AWS profile if required and run the Bash script.

```bash
export AWS_PROFILE="lab01"
cd ~/blog/003/0-bash/
./create-infrastructure.sh
```

## Discussion

The Bash script version has the lowest setup requirements on the client side. It only requires the [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html) installed. 

The script heavily uses the  `--query` parameter to retain the ID of objects to reference them later. Everything is executed sequential and there is no error handling in place. For example, what would happen if the script is run again with the infrastructure already in place.

# Python

## Execution

Create the Docker image:

```bash
docker build -f python3.docker -t mypython3 .
```

Run the container:

```bash
docker container run -it --name python3 --rm \
  --mount src=/<path-to-blog>/blog,target=/root/blog,type=bind \
  --mount src=/<path-to-aws-settings/.aws,target=/root/.aws,type=bind \
  mypython3 /bin/bash
```

Set the correct AWS profile if required and run the Bash script.

```bash
export AWS_PROFILE="lab01"
cd ~/blog/003/1-python
./create-infrastructure.py
```

## Discussion

Using Python instead of the Bash makes the code a bit simpler. For example, objects can be directly referenced when required. Otherwise, there is no significance between Bash and Python.

# Ansible

## Execution

Create the Docker image:

```bash
docker build -f ansible.docker -t myansible .
```

Run the container:

```bash
docker container run -it --name ansible-aws --rm \
  --mount src=/<path-to-code>/aws-labs,target=/root/aws-labs,type=bind \
  --mount src=/<path-to-aws-settings/.aws,target=/root/.aws,type=bind \
  myansible /bin/bash
```

Set the correct AWS profile and run the Ansible playbook:

```bash
export AWS_PROFILE="lab01"
cd ~/blog/003/2-ansible
ansible-playbook -i hosts.inv create-infrastructure.yml
```

## Discussion

The advantage of using Ansible is that the playbook is idempotent. You can run the same playbook multiple times without changing the result (existing resources are not changed). Ansible could also install and configure software on the created EC2 instances. So you need to learn and understand fewer tools.

Ansible allows splitting the activity (e.g. creating an EC2 instance) and the metadata (e.g. the name of the new instance). The activity goes into the playbook and the metadata into the variable file. This makes for a tidy setup.

# Terraform

## Execution

Create the Docker image:

```bash
docker build -f terraform.docker -t myterraform .
```

Run the container:

```bash
docker container run -it --name terraform --rm \
  --mount src=/<path-to-code>/aws-labs,target=/root/aws-labs,type=bind \
  --mount src=/<path-to-aws-settings/.aws,target=/root/.aws,type=bind \
  myterraform /bin/bash
```

Set the correct AWS profile and run the Ansible playbook:

```bash
cd ~/blog/003/3-terraform
terraform init
terraform apply
```

## Discussion

Terraform feels like the right approach to instantiate infrastructure. The configuration file is simple to understand. It's easy to reference other resources compared to Ansible. For example, have a look at the loop in Ansible playbook that extracts the IDs of the subnets so they can be used when creating the routing table.

Having all resources defined in the configuration, instead of looping through a variable to create subnets, seems to make the configuration also easier to read.

Terraform determines the dependencies of the different resources on each other. If they don't have any dependencies, Terraform instantiates them in parallel. This makes the provisioning faster and the configuration files simpler. For example, the order in which resources are defined is not relevant as Terraform determines the dependencies.

Terraform tracks also the state of resources. It executes steps to migrate the current state to the state new state defined in the configuration files. Further, the state tracking allows Terraform to destroy the infrastructure out of the box. No extra configuration or code is required for this.

Terraform supports different vendors and not just AWS. This is especially helpful if a service uses services from several cloud providers.

# CloudFormation

## Execution

Create the Docker image (this is the same as we used for the Bash example):

```bash
docker build -f ~/Data/docker/aws-cli.docker -t myawscli .
```

Run the container:

```bash
docker container run -it --name myawscli --rm \
  --mount src=/Users/roman/Data/code/aws-labs,target=/root/aws-labs,type=bind \
  --mount src=/Users/roman/.aws,target=/root/.aws,type=bind \
  myawscli /bin/bash
```

Set the correct AWS profile and run the Ansible playbook:

```bash
export AWS_PROFILE="lab01"
cd ~/blog/003/4-cloudformation
aws cloudformation create-stack --stack-name cloudformation \
  --template-body file://infrastructure.yml
```

## Discussion

The configuration file for CloudFormation is more verbose than the TerraForm one. For example, in CloudFormation you  first you define the route table and a route before adding the route to the route table. In Terraform, this a single resource declaration.

CloudFormation is obviously limited to AWS and doesn't support other cloud providers.

Empirically, the provisioning with CloudFormation took longer than with Terraform. I haven't measured the exact time. It was definitely more time consuming to troubleshoot configuration mistakes with CloudFormation. Many errors manifest very late during the provision of the resource. It appears that Terraform does more consistency checks early on.

# Conclusion

Of the above tools, Terraform is the best one for the infrastructure provisioning. CloudFormation is a close second because it doesn't support other providers and it requires a more verbose configuration file. 

Ansible is a valid option but does not provide the support for state as Terraform and CloudFormation does. Ansible appears to be stronger around configuration management. Consequently, Terraform is the right tool to provision the infrastructure to a point (i.e. creating the VM with a base system and Python installed) where Ansible can take over and configure the individual systems and services (i.e. installing packages and changing configuration files).